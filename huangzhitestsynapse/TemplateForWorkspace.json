{
	"$schema": "http://schema.management.azure.com/schemas/2015-01-01/deploymentTemplate.json#",
	"contentVersion": "1.0.0.0",
	"parameters": {
		"workspaceName": {
			"type": "string",
			"metadata": "Workspace name",
			"defaultValue": "huangzhitestsynapse"
		},
		"AzureDataLakeStorage1_accountKey": {
			"type": "secureString",
			"metadata": "Secure string for 'accountKey' of 'AzureDataLakeStorage1'"
		},
		"huangzhitestsynapse-WorkspaceDefaultSqlServer_connectionString": {
			"type": "secureString",
			"metadata": "Secure string for 'connectionString' of 'huangzhitestsynapse-WorkspaceDefaultSqlServer'"
		},
		"AzureDataLakeStorage1_properties_typeProperties_url": {
			"type": "string",
			"defaultValue": "https://huangzhistoragelogs.dfs.core.windows.net/"
		},
		"AzureDataLakeStorage2_properties_typeProperties_url": {
			"type": "string",
			"defaultValue": "https://huangzhistoragelogs.dfs.core.windows.net/"
		},
		"huangzhitestsynapse-WorkspaceDefaultStorage_properties_typeProperties_url": {
			"type": "string",
			"defaultValue": "https://huangzhitestadls.dfs.core.windows.net"
		}
	},
	"variables": {
		"workspaceId": "[concat('Microsoft.Synapse/workspaces/', parameters('workspaceName'))]"
	},
	"resources": [
		{
			"name": "[concat(parameters('workspaceName'), '/sparkpool2')]",
			"type": "Microsoft.Synapse/workspaces/bigDataPools",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": []
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/AzureDataLakeStorage1')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "AzureBlobFS",
				"typeProperties": {
					"url": "[parameters('AzureDataLakeStorage1_properties_typeProperties_url')]",
					"accountKey": {
						"type": "SecureString",
						"value": "[parameters('AzureDataLakeStorage1_accountKey')]"
					}
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/AzureDataLakeStorage2')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "AzureBlobFS",
				"typeProperties": {
					"url": "[parameters('AzureDataLakeStorage2_properties_typeProperties_url')]"
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/huangzhitestsynapse-WorkspaceDefaultSqlServer')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"parameters": {
					"DBName": {
						"type": "String"
					}
				},
				"annotations": [],
				"type": "AzureSqlDW",
				"typeProperties": {
					"connectionString": "[parameters('huangzhitestsynapse-WorkspaceDefaultSqlServer_connectionString')]"
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/huangzhitestsynapse-WorkspaceDefaultStorage')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "AzureBlobFS",
				"typeProperties": {
					"url": "[parameters('huangzhitestsynapse-WorkspaceDefaultStorage_properties_typeProperties_url')]"
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/AutoResolveIntegrationRuntime')]",
			"type": "Microsoft.Synapse/workspaces/integrationRuntimes",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"type": "Managed",
				"typeProperties": {
					"computeProperties": {
						"location": "AutoResolve",
						"dataFlowProperties": {
							"computeType": "General",
							"coreCount": 8,
							"timeToLive": 0
						}
					}
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/WorkspaceSystemIdentity')]",
			"type": "Microsoft.Synapse/workspaces/credentials",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"type": "ManagedIdentity",
				"typeProperties": {}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/CookPositive_20220713')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"nbformat": 4,
				"nbformat_minor": 2,
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"runAsWorkspaceSystemIdentity": false,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "32659328-7e12-4f97-9983-faddc64a17e7"
					}
				},
				"metadata": {
					"saveOutput": true,
					"synapse_widget": {
						"version": "0.1"
					},
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "python"
					},
					"language_info": {
						"name": "python"
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"%%configure -f\r\n",
							"{\r\n",
							"    \"driverMemory\": \"368g\",\r\n",
							"    \"driverCores\": 64,\r\n",
							"    \"executorMemory\": \"368g\",\r\n",
							"    \"numExecutors\": 14,\r\n",
							"    \"conf\": {\r\n",
							"        \"spark.speculation\": \"true\",\r\n",
							"        \"spark.stop.improvement.enabled\": \"false\",\r\n",
							"        \"spark.network.timeout\": 10000000, \r\n",
							"        \"spark.executor.heartbeatInterval\": 10000000,\r\n",
							"        \"spark.dynamicAllocation.maxExecutors\" : \"14\",\r\n",
							"        \"spark.dynamicAllocation.enable\": \"true\",\r\n",
							"        \"spark.dynamicAllocation.minExecutors\": \"2\"        \r\n",
							"    }\r\n",
							"}"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 1
					},
					{
						"cell_type": "code",
						"source": [
							"import numpy as np\r\n",
							"import pandas as pd\r\n",
							"import os\r\n",
							"\r\n",
							"import pyspark\r\n",
							"import pyspark.sql\r\n",
							"from pyspark.sql import DataFrame, SparkSession, functions, types, SQLContext\r\n",
							"from pyspark.sql.functions import col, date_add, expr, format_number, udf, when, hash, abs, pandas_udf, PandasUDFType\r\n",
							"from pyspark.sql.functions import when, expr\r\n",
							"from pyspark.sql.functions import * \r\n",
							"from pyspark.sql.functions import arrays_zip, col, explode, split\r\n",
							"from pyspark.sql import functions as F\r\n",
							"\r\n",
							"from pyspark.sql.types import * \r\n",
							"from pyspark.storagelevel import StorageLevel \r\n",
							"import scipy\r\n",
							"from sklearn import linear_model \r\n",
							"\r\n",
							"\r\n",
							"from datetime import datetime\r\n",
							"from datetime import timedelta\r\n",
							"\r\n",
							"from functools import reduce\r\n",
							"\r\n",
							"import datetime \r\n",
							"import dateutil\r\n",
							"\r\n",
							"from notebookutils import mssparkutils"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 2
					},
					{
						"cell_type": "code",
						"source": [
							"#spark.conf.get(\"spark.speculation\")"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 3
					},
					{
						"cell_type": "code",
						"source": [
							"%run \"/chenjzha/data_utils_v2\""
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 4
					},
					{
						"cell_type": "code",
						"metadata": {
							"tags": [
								"parameters"
							]
						},
						"source": [
							"timeBegin = ''\r\n",
							"timeEnd = ''\r\n",
							"maxLeadingHour = ''\r\n",
							"engineeringHourRange = ''"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 5
					},
					{
						"cell_type": "code",
						"source": [
							"timeBegin = dateutil.parser.parse(timeBegin)\r\n",
							"timeEnd = dateutil.parser.parse(timeEnd)\r\n",
							"outputPathSuffix = f\"({timeBegin.year}_{timeBegin.month}_{timeBegin.day})({timeEnd.year}_{timeEnd.month}_{timeEnd.day})L{maxLeadingHour}h\""
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 7
					},
					{
						"cell_type": "code",
						"source": [
							"try:\r\n",
							"    mssparkutils.fs.mount(  \r\n",
							"        \"abfss://featuredata@onelinedfpadls2.dfs.core.windows.net\",  \r\n",
							"        \"/data\",\r\n",
							"        {\"linkedService\":\"onlinefeaturedata\"} \r\n",
							"    ) \r\n",
							"except:\r\n",
							"    print(\"already exist mount point\")\r\n",
							"    pass"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 8
					},
					{
						"cell_type": "code",
						"source": [
							"KUSTO_CLUSTER = 'https://o365exoha.kusto.windows.net'\r\n",
							"KUSTO_DATABASE = 'O365ExoHAMon'\r\n",
							"KUSTO_DATABASE_TEST = 'O365ExoHAMonPPE'\r\n",
							"\r\n",
							"KUSTO_AAD_APP_ID = \"4cdae237-e303-4aa2-a1c4-ff067ca0b662\"\r\n",
							"KUSTO_AAD_APP_SECRET = mssparkutils.credentials.getSecretWithLS(linkedService=\"AzureKeyVault1\", secret=\"EXO-TORUS-APP-SECRET\")\r\n",
							"KUSTO_AAD_AUTHORITY_ID = \"cdc5aeea-15c5-4db6-b079-fcadd2505dc2\"\r\n",
							"\r\n",
							"\r\n",
							"kusto_loader = spark.read \\\r\n",
							"    .format(\"com.microsoft.kusto.spark.synapse.datasource\") \\\r\n",
							"    .option(\"kustoAadAppId\", KUSTO_AAD_APP_ID) \\\r\n",
							"    .option(\"kustoAadAppSecret\", KUSTO_AAD_APP_SECRET)  \\\r\n",
							"    .option(\"kustoAadAuthorityID\", KUSTO_AAD_AUTHORITY_ID) \\\r\n",
							"    .option(\"kustoCluster\", KUSTO_CLUSTER) \\\r\n",
							"    .option(\"kustoDatabase\", KUSTO_DATABASE)"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 9
					},
					{
						"cell_type": "code",
						"source": [
							"sampleID = 2\r\n",
							""
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 10
					},
					{
						"cell_type": "code",
						"source": [
							"def getDiskFailureV4(startDate,endDate):\r\n",
							"    startDate = startDate.strftime(\"%Y-%m-%d\")\r\n",
							"    endDate = endDate.strftime(\"%Y-%m-%d\")\r\n",
							"    query = '''let R = RepairBoxDiskFailures_v4_basedV3\r\n",
							"| where CreatedTime < todatetime('endDate')\r\n",
							"| where CreatedTime >= todatetime('startDate')\r\n",
							"| where isnotempty(TicketId) and isnotempty(SerialNumber) and Data !contains \"Missing physicaldrive\"\r\n",
							"| project SerialNumber,MachineName,CreatedTime;\r\n",
							"R'''\r\n",
							"    query = query.replace('startDate',startDate)\r\n",
							"    query = query.replace('endDate',endDate)\r\n",
							"    kustoDF = kusto_loader.option(\"kustoQuery\", query).load()\r\n",
							"    kustoDF = kustoDF.withColumnRenamed('MachineName', 'env_cloud_roleInstance')\r\n",
							"    return kustoDF"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 11
					},
					{
						"cell_type": "code",
						"source": [
							"def getDiskFailureV3orV2(startDate,endDate):\r\n",
							"    startDate = startDate.strftime(\"%Y-%m-%d\")\r\n",
							"    endDate = endDate.strftime(\"%Y-%m-%d\")\r\n",
							"    query = '''let R = RepairBoxDiskFailures_v3\r\n",
							"| where CreatedTime >= todatetime('startDate')\r\n",
							"| where CreatedTime < todatetime('endDate')\r\n",
							"| where isnotempty(SerialNumber) and DiskLocation contains 'HDD'\r\n",
							"| project env_cloud_roleInstance = MachineName, SerialNumber, Type, CreatedTime, DiskLocation\r\n",
							"| join kind=fullouter (\r\n",
							"    RepairBoxDiskFailures_v2\r\n",
							"    | where CreatedTime >= todatetime('startDate')\r\n",
							"    | where CreatedTime < todatetime('endDate')\r\n",
							"    | where isnotempty(SerialNumber) and LabelLocation contains 'HDD'\r\n",
							"    | project env_cloud_roleInstance, SerialNumber, Type, CreatedTime, DiskLocation = LabelLocation\r\n",
							"    )\r\n",
							"    on env_cloud_roleInstance, CreatedTime, DiskLocation and Type\r\n",
							"| project   env_cloud_roleInstance = iff(isnotempty(env_cloud_roleInstance), env_cloud_roleInstance, env_cloud_roleInstance1),\r\n",
							"            SerialNumber = iff(isnotempty(SerialNumber), SerialNumber, SerialNumber1),\r\n",
							"            CreatedTime = iff(isnotempty(CreatedTime), CreatedTime, CreatedTime1);\r\n",
							"R'''\r\n",
							"    query = query.replace('startDate',startDate)\r\n",
							"    query = query.replace('endDate',endDate)\r\n",
							"    kustoDF = kusto_loader.option(\"kustoQuery\", query).load()\r\n",
							"    return kustoDF"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 12
					},
					{
						"cell_type": "code",
						"source": [
							"def getDiskFailureV5(startDate,endDate):\r\n",
							"    startDate = startDate.strftime(\"%Y-%m-%d\")\r\n",
							"    endDate = endDate.strftime(\"%Y-%m-%d\")\r\n",
							"    query = '''let R = RepairBoxDiskFailures_v5\r\n",
							"| where CreatedTime >= todatetime('startDate')\r\n",
							"| where CreatedTime < todatetime('endDate')\r\n",
							"| where isnotempty(SerialNumber) and DiskLocation contains 'HDD'\r\n",
							"| project env_cloud_roleInstance = MachineName, SerialNumber, CreatedTime;\r\n",
							"R'''\r\n",
							"    query = query.replace('startDate',startDate)\r\n",
							"    query = query.replace('endDate',endDate)\r\n",
							"    kustoDF = kusto_loader.option(\"kustoQuery\", query).load()\r\n",
							"    return kustoDF"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 13
					},
					{
						"cell_type": "code",
						"source": [
							"def If_Blob_exist(hourly_time, path_prefix = \"\"):\r\n",
							"    path_suffix = f\"{path_prefix}/{hourly_time.year}/{hourly_time.month}/{hourly_time.day}/{hourly_time.hour}\"\r\n",
							"    # print(path_suffix)\r\n",
							"    jobId = mssparkutils.env.getJobId() \r\n",
							"    # print(jobId)\r\n",
							"    blob_exist =  os.path.exists(f\"/synfs/{jobId}/data/predictiondata/{path_suffix}\")\r\n",
							"    #print(blob_exist, f\"{jobId}/data/predictiondata/{path_suffix}\")\r\n",
							"    return blob_exist, f\"{jobId}/data/predictiondata/{path_suffix}\"\r\n",
							"\r\n",
							"def save_Blob(KustoDF, pathSuffix):\r\n",
							"\r\n",
							"    jobId = mssparkutils.env.getJobId() \r\n",
							"    # print(jobId)  \r\n",
							"    KustoDF.coalesce(1).write.mode(\"overwrite\").csv(f\"synfs:/{jobId}/data/test/Cooked/Positive/{pathSuffix}\",header=True)\r\n",
							"    "
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 14
					},
					{
						"cell_type": "code",
						"source": [
							"def extractRawData(diskFailures, pathPrefix):\r\n",
							"    dataBeginTime = timeBegin - datetime.timedelta(hours = maxLeadingHour + engineeringHourRange)\r\n",
							"    timeDiff = timeEnd - timeBegin\r\n",
							"    hourTotal = timeDiff.days * 24 + maxLeadingHour + engineeringHourRange\r\n",
							"    \r\n",
							"    pathList = []\r\n",
							"    for i in range(0, hourTotal):\r\n",
							"        currentTime = dataBeginTime + datetime.timedelta(hours=i)\r\n",
							"        #print(currentTime)\r\n",
							"        #pathSuffix = f\"{currentTime.year}/{currentTime.month}/{currentTime.day}/{currentTime.hour}\"\r\n",
							"        isExist, current_path = If_Blob_exist(currentTime, pathPrefix)\r\n",
							"        if(isExist):\r\n",
							"            pathList.append(current_path)\r\n",
							"        else:\r\n",
							"            print(f'{isExist} {current_path}')\r\n",
							"            #mssparkutils.notebook.exit(0)\r\n",
							"\r\n",
							"    filePaths = export_dirs_to_paths(pathList)\r\n",
							"\r\n",
							"    pySpark = SparkSession.builder.appName(\"ReadPySpark\").getOrCreate()   \r\n",
							"    rawData = pySpark.read.csv(path=filePaths, header=True, inferSchema=True)\r\n",
							"\r\n",
							"    rawData = rawData.withColumn(\"BinPreciseTimeStamp\", to_timestamp(col(\"BinPreciseTimeStamp\")))\r\n",
							"    rawData = rawData.withColumn('BinPreciseTimeStamp', date_trunc('hour', rawData.BinPreciseTimeStamp))\r\n",
							"\r\n",
							"    rawData = diskFailures.join(rawData, on = ['env_cloud_roleInstance', 'SerialNumber', 'BinPreciseTimeStamp'], how = 'inner')\r\n",
							"\r\n",
							"    #rawData = rawData.repartition('env_cloud_roleInstance', 'SerialNumber', 'FirstPreciseTimeStamp')\r\n",
							"\r\n",
							"    rawData = rawData.drop_duplicates().replace(float('nan'), None)\r\n",
							"    rawData = rawData.withColumn(\"SampleID\", lit(sampleID)).withColumn(\"Label\", lit(True))\r\n",
							"\r\n",
							"    return rawData"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 15
					},
					{
						"cell_type": "code",
						"source": [
							"def resampleDiskFailure(diskFailures):\r\n",
							"    diskFailures = diskFailures.withColumn('CreatedTime', date_trunc('hour', diskFailures.CreatedTime))\r\n",
							"    diskFailures = diskFailures.withColumnRenamed('CreatedTime', 'FirstPreciseTimeStamp')\r\n",
							"    resampledDiskFailures = diskFailures.alias('resampledDiskFailures')\r\n",
							"\r\n",
							"    for leadingTime in range(1, maxLeadingHour):\r\n",
							"        diskFailures = diskFailures.withColumn('FirstPreciseTimeStamp', diskFailures.FirstPreciseTimeStamp - F.expr('INTERVAL 1 HOURS'))\r\n",
							"        resampledDiskFailures = resampledDiskFailures.unionByName(diskFailures, allowMissingColumns = False)\r\n",
							"\r\n",
							"    resampledDiskFailures = resampledDiskFailures.withColumn('BinPreciseTimeStamp', resampledDiskFailures.FirstPreciseTimeStamp)\r\n",
							"    diskFailures = resampledDiskFailures.alias('diskFailures')\r\n",
							"\r\n",
							"    for dataHour in range(1, engineeringHourRange):\r\n",
							"        diskFailures = diskFailures.withColumn('BinPreciseTimeStamp', diskFailures.BinPreciseTimeStamp - F.expr('INTERVAL 1 HOURS'))\r\n",
							"\r\n",
							"        resampledDiskFailures = resampledDiskFailures.unionByName(diskFailures, allowMissingColumns = False)\r\n",
							"    \r\n",
							"    # FirstPreciseTimeStamp is moved forward 2h in prediction, and moved back 2h before feature engineering.\r\n",
							"    resampledDiskFailures = resampledDiskFailures.withColumn('FirstPreciseTimeStamp', resampledDiskFailures.FirstPreciseTimeStamp + F.expr('INTERVAL 2 HOURS'))\r\n",
							"\r\n",
							"    resampledDiskFailures = resampledDiskFailures.repartition('env_cloud_roleInstance', 'SerialNumber', 'FirstPreciseTimeStamp')\r\n",
							"\r\n",
							"    return resampledDiskFailures"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 16
					},
					{
						"cell_type": "code",
						"source": [
							"diskFailures = getDiskFailureV5(timeBegin, timeEnd).drop_duplicates()"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 17
					},
					{
						"cell_type": "code",
						"source": [
							"resampledDiskFailures = resampleDiskFailure(diskFailures)"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 18
					},
					{
						"cell_type": "code",
						"source": [
							"def getFeatureSMART():\r\n",
							"    rawSmart = extractRawData(resampledDiskFailures, 'DiskSmartAttributeHourly')\r\n",
							"\r\n",
							"    rawSmart = rawSmart.filter(rawSmart['AttributeName'].isNotNull())\r\n",
							"\r\n",
							"    # TODO: emergency fix 20211230 \r\n",
							"    rawSmart = rawSmart.withColumn('AttributeName', regexp_replace('AttributeName', 'Power-off_Retract_Count', 'Power-Off_Retract_Count'))\r\n",
							"\r\n",
							"    # may contain incorrect env_cloud_name == O365Passive, which will result in duplicated entries, one for each env_cloud_name, of the same disk in the engineered feature.\r\n",
							"    rawSmart = rawSmart.withColumn('env_cloud_name', lit(''))\r\n",
							"\r\n",
							"    #save_Blob(rawSmart, 'RawSMART')\r\n",
							"\r\n",
							"    featureSMART = get_common_features(None, rawSmart, smart_split_rows, SMART_Atts, ResultSchema)\r\n",
							"\r\n",
							"    return featureSMART\r\n",
							""
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 19
					},
					{
						"cell_type": "code",
						"source": [
							"featureSMART = getFeatureSMART()"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 20
					},
					{
						"cell_type": "code",
						"source": [
							"#mssparkutils.notebook.exit(0)"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 21
					},
					{
						"cell_type": "code",
						"source": [
							"def getFeaturePC():\r\n",
							"    rawPC = extractRawData(resampledDiskFailures, 'WindowsDiskPerfCounterHourly')\r\n",
							"    #save_Blob(rawPC, 'RawPC')\r\n",
							"    featurePC = get_perf_counter_features(None, rawPC, perf_counter_split_rows, PerfCounterFinalAttrNames, ResultSchema)\r\n",
							"    featurePC = featurePC.withColumnRenamed(\"env_cloud_name\", \"env_cloud_name_p\")\r\n",
							"    return featurePC"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 22
					},
					{
						"cell_type": "code",
						"source": [
							"featurePC = getFeaturePC()"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 23
					},
					{
						"cell_type": "code",
						"source": [
							"def getFeatureEvent():\r\n",
							"    rawEvent = extractRawData(resampledDiskFailures, 'DiskEventCounter')\r\n",
							"\r\n",
							"    # may contain incorrect env_cloud_name == O365Passive, which will result in duplicated entries, one for each env_cloud_name, of the same disk in the engineered feature.\r\n",
							"    rawEvent = rawEvent.withColumn('env_cloud_name', lit(''))\r\n",
							"\r\n",
							"    #save_Blob(rawEvent, 'RawEvent')\r\n",
							"\r\n",
							"    featureEvent = get_event_features(None, rawEvent)\r\n",
							"    featureEvent = featureEvent.withColumnRenamed(\"env_cloud_name\", \"env_cloud_name_e\")\r\n",
							"\r\n",
							"    return featureEvent"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 24
					},
					{
						"cell_type": "code",
						"source": [
							"featureEvent = getFeatureEvent()"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 25
					},
					{
						"cell_type": "code",
						"source": [
							"#mssparkutils.notebook.exit(0)"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 26
					},
					{
						"cell_type": "code",
						"source": [
							"featureSmartPc = featureSMART.join(featurePC, on = ['env_cloud_roleInstance', 'SerialNumber', 'FirstPreciseTimeStamp', 'SampleID', 'Label'], how = 'full')\r\n",
							"featureSmartPcEvent = featureSmartPc.join(featureEvent, on = ['env_cloud_roleInstance', 'SerialNumber', 'FirstPreciseTimeStamp', 'SampleID', 'Label'], how = 'full')\r\n",
							"featureSmartPcEvent = featureSmartPcEvent.replace(float('nan'), None)"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 27
					},
					{
						"cell_type": "code",
						"source": [
							"for column in selected_column_names_HDFV1:\r\n",
							"    if column not in featureSmartPcEvent.columns:\r\n",
							"        featureSmartPcEvent = featureSmartPcEvent.withColumn(column, lit(''))\r\n",
							"        print(f'add missing column {column}')\r\n",
							"featureSmartPcEvent = featureSmartPcEvent.select(selected_column_names_HDFV1)"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 28
					},
					{
						"cell_type": "code",
						"source": [
							"#featureSmartPcEvent.coalesce(1).write.mode(\"overwrite\").format(\"com.databricks.spark.csv\").option(\"header\", \"true\").save(f\"dbfs:/mnt/featuredatablob/Dataset/20220520/{outputPathSuffix}\")\r\n",
							"\r\n",
							"#save_Blob(featureSmartPcEvent, outputPathSuffix)"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 29
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"def getGen7():\r\n",
							"    query = '''let R = MachineInfo\r\n",
							"| where SKU contains 'Gen7'\r\n",
							"| distinct MachineName\r\n",
							"| project env_cloud_roleInstance = MachineName;\r\n",
							"R'''\r\n",
							"    kustoDF = kusto_loader.option(\"kustoQuery\", query).load()\r\n",
							"    return kustoDF\r\n",
							"\r\n",
							"\r\n",
							"def removeGen7(disks):\r\n",
							"    gen7 = getGen7()\r\n",
							"    disks = disks.join(gen7, on = ['env_cloud_roleInstance'], how = 'leftanti')\r\n",
							"    return disks"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"featureSmartPcEvent = removeGen7(featureSmartPcEvent)\r\n",
							"featureSmartPcEvent = featureSmartPcEvent.select(selected_column_names_HDFV1)"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"save_Blob(featureSmartPcEvent, outputPathSuffix)"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Notebook 1')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "sparkpool",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "56g",
					"driverCores": 8,
					"executorMemory": "56g",
					"executorCores": 8,
					"numExecutors": 2,
					"runAsWorkspaceSystemIdentity": false,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "7aef3995-1b75-473d-8a06-48665c120522"
					}
				},
				"metadata": {
					"saveOutput": true,
					"synapse_widget": {
						"version": "0.1"
					},
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/76ebc6d1-182c-411a-9b4d-aed0242f8a82/resourceGroups/huangzhitest/providers/Microsoft.Synapse/workspaces/huangzhitestsynapse/bigDataPools/sparkpool",
						"name": "sparkpool",
						"type": "Spark",
						"endpoint": "https://huangzhitestsynapse.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/sparkpool",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net",
							"authHeader": null
						},
						"sparkVersion": "3.2",
						"nodeCount": 3,
						"cores": 8,
						"memory": 56,
						"extraHeader": null
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"mssparkutils.fs.mount( \r\n",
							"    \"abfss://dataset@huangzhitestadls.dfs.core.windows.net\", \r\n",
							"    \"/test\", \r\n",
							"    {\"linkedService\":\"huangzhitestsynapse-WorkspaceDefaultStorage\"} \r\n",
							")\r\n",
							"\r\n",
							"jobId = mssparkutils.env.getJobId()\r\n",
							""
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 51
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"import os\r\n",
							"import json\r\n",
							"metastore_dir = \"/home/trusted-service-user/tmp_metadata\"\r\n",
							"metastore_view_dir =  \"/home/trusted-service-user/tmp_metadata_views\"\r\n",
							"default_database_location = \"dbfs:/user/hive/warehouse\""
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# copy table DDL into a local location.\r\n",
							"import shutil\r\n",
							"if (os.path.exists(metastore_dir)):\r\n",
							"    shutil.rmtree(metastore_dir)\r\n",
							"shutil.copytree(f\"/synfs/{jobId}/test/spark_metadata\", \"/home/trusted-service-user/tmp_metadata\")"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 106
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"os.listdir('/home/trusted-service-user/tmp_metadata/default')"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 112
					},
					{
						"cell_type": "code",
						"source": [
							""
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 90
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"def get_database_detail_dict(db_log='database_details.log'):\r\n",
							"    db_logfile = os.path.join(metastore_dir, db_log)\r\n",
							"    all_db_json = {}\r\n",
							"    with open(db_logfile, 'r') as fp:\r\n",
							"        for x in fp:\r\n",
							"            db_json = json.loads(x)\r\n",
							"            db_name = db_json.pop('Namespace Name')\r\n",
							"            all_db_json[db_name] = db_json\r\n",
							"    return all_db_json\r\n",
							"\r\n",
							"\r\n",
							"def create_database_db(db_name, db_attributes):\r\n",
							"    location = db_attributes.get('Location', '')\r\n",
							"    if not location.startswith(default_database_location):\r\n",
							"        create_stmt = f\"CREATE DATABASE IF NOT EXISTS {db_name} LOCATION '{location}'\"\r\n",
							"    else:\r\n",
							"        create_stmt = f\"CREATE DATABASE IF NOT EXISTS {db_name}\"\r\n",
							"    # print(create_stmt)\r\n",
							"    db_results = spark.sql(create_stmt)\r\n",
							"    return db_results\r\n",
							"\r\n",
							"def get_ddl_by_keyword_group(local_path): \r\n",
							"    \"\"\"\r\n",
							"    return a list of DDL strings that are grouped by keyword arguments and their parameters\r\n",
							"    \"\"\"\r\n",
							"    ddl_statement = []\r\n",
							"    parameter_group = []\r\n",
							"    with open(local_path, 'r') as fp:\r\n",
							"        for line in fp:\r\n",
							"            raw = line.rstrip()\r\n",
							"            if not raw:\r\n",
							"                # make sure it's not an empty line, continue if empty\r\n",
							"                continue\r\n",
							"            if raw[0] == ' ' or raw[0] == ')':\r\n",
							"                parameter_group.append(raw)\r\n",
							"            else:\r\n",
							"                if parameter_group:\r\n",
							"                    ddl_statement.append(''.join(parameter_group))\r\n",
							"                parameter_group = [raw]\r\n",
							"        ddl_statement.append(''.join(parameter_group)) \r\n",
							"    return ddl_statement\r\n",
							"\r\n",
							"def is_ddl_a_view(ddl_list):\r\n",
							"    first_statement = ddl_list[0]\r\n",
							"    if first_statement.startswith('CREATE VIEW'):\r\n",
							"        return True\r\n",
							"    return False\r\n",
							"\r\n",
							"def move_table_view(db_name, tbl_name, local_table_ddl):\r\n",
							"    ddl_statement = get_ddl_by_keyword_group(local_table_ddl)\r\n",
							"    if is_ddl_a_view(ddl_statement):\r\n",
							"        dst_local_ddl = os.path.join(metastore_view_dir, db_name, tbl_name)\r\n",
							"        os.rename(local_table_ddl, dst_local_ddl)\r\n",
							"        return True\r\n",
							"    return False\r\n",
							"\r\n",
							"def get_path_option_if_available(stmt):\r\n",
							"    # parse the OPTIONS keyword and pull out the `path` parameter if it exists\r\n",
							"    import re\r\n",
							"    params = re.search(r'\\((.*?)\\)', stmt).group(1) # \r\n",
							"    params_list = list(map(lambda p: p.lstrip().rstrip(), params.split(','))) # \r\n",
							"    for x in params_list:\r\n",
							"        if x.startswith('path'):\r\n",
							"            return f'OPTIONS ( {x} )' # \r\n",
							"    return ''    \r\n",
							"\r\n",
							"def is_table_location_defined(local_table_path): \r\n",
							"    \"\"\" check if LOCATION or OPTIONS(path ..) are defined for the table\r\n",
							"    \"\"\"\r\n",
							"    ddl_statement = get_ddl_by_keyword_group(local_table_path) \r\n",
							"    for keyword_param in ddl_statement: \r\n",
							"        if keyword_param.startswith('OPTIONS'): \r\n",
							"            options_param = get_path_option_if_available(keyword_param) \r\n",
							"            if options_param:\r\n",
							"                # if the return is not empty, the path option is provided which means its an external table\r\n",
							"                return True\r\n",
							"        elif keyword_param.startswith('LOCATION'):\r\n",
							"            # if LOCATION is defined, we know the external table location\r\n",
							"            return True\r\n",
							"    return False\r\n",
							"\r\n",
							"def update_table_ddl(local_table_path, db_path):\r\n",
							"    # check if the database location / path is the default DBFS path\r\n",
							"    table_name = os.path.basename(local_table_path) # local_table_path的basename就是table_name\r\n",
							"    is_db_default_path = db_path.startswith(default_database_location) #\r\n",
							"    if (not is_db_default_path) and (not is_table_location_defined(local_table_path)):\r\n",
							"        # the LOCATION attribute is not defined and the Database has a custom location defined\r\n",
							"        # therefore we need to add it to the DDL, e.g. dbfs:/db_path/table_name\r\n",
							"        table_path = os.path.join(db_path, table_name)\r\n",
							"        location_stmt = f\"\\nLOCATION '{table_path}'\"\r\n",
							"        with open(local_table_path, 'a') as fp:\r\n",
							"            fp.write(location_stmt)\r\n",
							"        return True\r\n",
							"    return False\r\n",
							"\r\n",
							"def is_delta_table(local_path):\r\n",
							"    with open(local_path, 'r') as fp:\r\n",
							"        for line in fp:\r\n",
							"            lower_line = line.lower()\r\n",
							"            if lower_line.startswith('using delta'):\r\n",
							"                return True\r\n",
							"    return False\r\n",
							"\r\n",
							"def get_local_tmp_ddl_if_applicable(current_local_ddl_path):\r\n",
							"    return current_local_ddl_path\r\n",
							"\r\n",
							"def apply_table_ddl(local_table_path, db_path):\r\n",
							"    \"\"\"\r\n",
							"    Run DDL command on destination workspace\r\n",
							"    :param local_table_path: local file path to the table DDL\r\n",
							"    :param db_path: database S3 / Blob Storage / ADLS path for the Database\r\n",
							"    \"\"\"\r\n",
							"    updated_table_status = update_table_ddl(local_table_path, db_path) # \r\n",
							"\r\n",
							"    # update local table ddl to a new temp file with OPTIONS and TBLPROPERTIES removed from the DDL for delta tables\r\n",
							"    if is_delta_table(local_table_path):\r\n",
							"        local_table_path = get_local_tmp_ddl_if_applicable(local_table_path)\r\n",
							"\r\n",
							"    with open(local_table_path, \"r\") as fp:\r\n",
							"        ddl_statement = fp.read()\r\n",
							"        #print(ddl_statement)\r\n",
							"        try:\r\n",
							"            ddl_results = spark.sql(ddl_statement)\r\n",
							"        except Exception as e:\r\n",
							"            if \"mstest\" in ddl_statement:\r\n",
							"                print(e)\r\n",
							"           \r\n",
							"def delete_dir_if_empty(local_dir):\r\n",
							"    \r\n",
							"    if os.path.exists(local_dir) and len(os.listdir(local_dir)) == 0:\r\n",
							"        os.rmdir(local_dir)           "
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 150
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"print(tables)"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 151
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"\r\n",
							"db_list = get_database_detail_dict()\r\n",
							"\r\n",
							"os.makedirs(metastore_view_dir, exist_ok=True)\r\n",
							"\r\n",
							"# recreate tables\r\n",
							"\r\n",
							"for db_name in db_list:\r\n",
							"    # create a dir to host the view ddl if we find them\r\n",
							"    os.makedirs(os.path.join(metastore_view_dir, db_name), exist_ok=True)\r\n",
							"    # get the local database path to list tables\r\n",
							"    local_db_path = os.path.join(metastore_dir, db_name)\r\n",
							"    # get a dict of the database attributes\r\n",
							"    database_attributes = all_db_details_json.get(db_name, '')\r\n",
							"    create_database_db(db_name, database_attributes)\r\n",
							"    db_path = database_attributes.get('Location')\r\n",
							"    tables = os.listdir(local_db_path)\r\n",
							"    print(tables)\r\n",
							"    for tbl_name in tables:\r\n",
							"        \r\n",
							"        local_table_ddl = os.path.join(local_db_path, tbl_name)\r\n",
							"        if tbl_name == \"mstest22\":\r\n",
							"            print(local_table_ddl)\r\n",
							"        if not move_table_view(db_name, tbl_name, local_table_ddl):\r\n",
							"            # we hit a table ddl here, so we apply the ddl\r\n",
							"            is_successful = apply_table_ddl(local_table_ddl, db_path)\r\n",
							"        else:\r\n",
							"            print(f'Moving view ddl to re-apply later: {db_name}.{tbl_name}')   \r\n",
							"    delete_dir_if_empty(os.path.join(metastore_view_dir, db_name))\r\n",
							"\r\n",
							""
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 152
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"CREATE TABLE default.mstest22 (\r\n",
							"  quarterlymeterdatafromdate TIMESTAMP,\r\n",
							"  `input_file_name` STRING)\r\n",
							"USING parquet\r\n",
							"LOCATION 'abfss://dataset@huangzhitestadls.dfs.core.windows.net/mstest2'"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 154
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# recreate views\r\n",
							"views_db_list = os.listdir(metastore_view_dir)\r\n",
							"for db_name in views_db_list:\r\n",
							"    local_view_db_path = os.path.join(metastore_view_dir, db_name)\r\n",
							"    database_attributes = all_db_details_json.get(db_name, '')\r\n",
							"    db_path = database_attributes.get('Location')\r\n",
							"    if os.path.isdir(local_view_db_path):\r\n",
							"        views = os.listdir(local_view_db_path)\r\n",
							"        for view_name in views:\r\n",
							"            local_view_ddl = os.path.join(metastore_view_dir, db_name, '/', view_name)\r\n",
							"            is_successful = apply_table_ddl(local_view_ddl, db_path)\r\n",
							""
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 109
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"os.listdir(\"/home/trusted-service-user/tmp_metadata_views\")"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 131
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"show tables"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 149
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"select * from mstest22"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 155
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							""
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							""
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 26
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"jobId = mssparkutils.env.getJobId()"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 16
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"import os\r\n",
							"os.listdir(f\"/synfs/{jobId}/test/synapsemetadata/metadata\")"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 17
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							" "
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 8
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							""
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 9
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							""
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 14
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"\r\n",
							""
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 5
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							""
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Notebook 2')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "sparkpool2",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"runAsWorkspaceSystemIdentity": false,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "648b7aeb-6640-43e2-9e27-6a95a2fbc191"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/76ebc6d1-182c-411a-9b4d-aed0242f8a82/resourceGroups/huangzhitest/providers/Microsoft.Synapse/workspaces/huangzhitestsynapse/bigDataPools/sparkpool2",
						"name": "sparkpool2",
						"type": "Spark",
						"endpoint": "https://huangzhitestsynapse.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/sparkpool2",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.2",
						"nodeCount": 3,
						"cores": 4,
						"memory": 28,
						"automaticScaleJobs": false
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"metadata": {
							"microsoft": {
								"language": "python"
							},
							"collapsed": false
						},
						"source": [
							"%%pyspark\r\n",
							"spark.conf.set(\"spark.storage.synapse.huangzhistoragelogs.dfs.core.windows.net.linkedServiceName\", \"AzureDataLakeStorage1\")\r\n",
							"spark.conf.set(\"fs.azure.account.oauth.provider.type.huangzhistoragelogs.dfs.core.windows.net\",\"com.microsoft.azure.synapse.tokenlibrary.LinkedServiceBasedTokenProvider\")\r\n",
							"\r\n",
							"df = spark.read.csv('abfss://dataset@huangzhistoragelogs.dfs.core.windows.net/mstest/')\r\n",
							"display(df.limit(10))"
						],
						"outputs": [],
						"execution_count": 2
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "python"
							},
							"collapsed": false
						},
						"source": [
							"%%pyspark\r\n",
							"spark.conf.set(\"spark.storage.synapse.huangzhistoragelogs.dfs.core.windows.net.linkedServiceName\", \"AzureDataLakeStorage2\")\r\n",
							"spark.conf.set(\"fs.azure.account.oauth.provider.type.huangzhistoragelogs.dfs.core.windows.net\",\"com.microsoft.azure.synapse.tokenlibrary.LinkedServiceBasedTokenProvider\")\r\n",
							"\r\n",
							"df = spark.read.csv('abfss://dataset@huangzhistoragelogs.dfs.core.windows.net/mstest/')\r\n",
							"display(df.limit(10))"
						],
						"outputs": [],
						"execution_count": 10
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							""
						],
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/import_tables')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"nbformat": 4,
				"nbformat_minor": 2,
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"runAsWorkspaceSystemIdentity": false,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "8d178405-0f19-4c80-b6e8-54c4ac858359"
					}
				},
				"metadata": {
					"saveOutput": true,
					"synapse_widget": {
						"version": "0.1"
					},
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "python"
					},
					"language_info": {
						"name": "python"
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"mssparkutils.fs.mount( \r\n",
							"    \"abfss://dataset@huangzhitestadls.dfs.core.windows.net\", \r\n",
							"    \"/test\", \r\n",
							"    {\"linkedService\":\"huangzhitestsynapse-WorkspaceDefaultStorage\"} \r\n",
							")\r\n",
							"\r\n",
							"jobId = mssparkutils.env.getJobId()"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"import os\r\n",
							"import json\r\n",
							"metastore_dir = \"/home/trusted-service-user/tmp_metadata\"\r\n",
							"metastore_view_dir =  \"/home/trusted-service-user/tmp_metadata_views\"\r\n",
							"default_database_location = \"dbfs:/user/hive/warehouse\""
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# copy table DDL into a local location.\r\n",
							"import shutil\r\n",
							"if (os.path.exists(metastore_dir)):\r\n",
							"    shutil.rmtree(metastore_dir)\r\n",
							"shutil.copytree(f\"/synfs/{jobId}/test/spark_metadata\", \"/home/trusted-service-user/tmp_metadata\")"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"def get_database_detail_dict(db_log='database_details.log'):\r\n",
							"    db_logfile = os.path.join(metastore_dir, db_log)\r\n",
							"    all_db_json = {}\r\n",
							"    with open(db_logfile, 'r') as fp:\r\n",
							"        for x in fp:\r\n",
							"            db_json = json.loads(x)\r\n",
							"            db_name = db_json.pop('Namespace Name')\r\n",
							"            all_db_json[db_name] = db_json\r\n",
							"    return all_db_json\r\n",
							"\r\n",
							"\r\n",
							"def create_database_db(db_name, db_attributes):\r\n",
							"    location = db_attributes.get('Location', '')\r\n",
							"    if not location.startswith(default_database_location):\r\n",
							"        create_stmt = f\"CREATE DATABASE IF NOT EXISTS {db_name} LOCATION '{location}'\"\r\n",
							"    else:\r\n",
							"        create_stmt = f\"CREATE DATABASE IF NOT EXISTS {db_name}\"\r\n",
							"    # print(create_stmt)\r\n",
							"    db_results = spark.sql(create_stmt)\r\n",
							"    return db_results\r\n",
							"\r\n",
							"def get_ddl_by_keyword_group(local_path): \r\n",
							"    \"\"\"\r\n",
							"    return a list of DDL strings that are grouped by keyword arguments and their parameters\r\n",
							"    \"\"\"\r\n",
							"    ddl_statement = []\r\n",
							"    parameter_group = []\r\n",
							"    with open(local_path, 'r') as fp:\r\n",
							"        for line in fp:\r\n",
							"            raw = line.rstrip()\r\n",
							"            if not raw:\r\n",
							"                # make sure it's not an empty line, continue if empty\r\n",
							"                continue\r\n",
							"            if raw[0] == ' ' or raw[0] == ')':\r\n",
							"                parameter_group.append(raw)\r\n",
							"            else:\r\n",
							"                if parameter_group:\r\n",
							"                    ddl_statement.append(''.join(parameter_group))\r\n",
							"                parameter_group = [raw]\r\n",
							"        ddl_statement.append(''.join(parameter_group)) \r\n",
							"    return ddl_statement\r\n",
							"\r\n",
							"def is_ddl_a_view(ddl_list):\r\n",
							"    first_statement = ddl_list[0]\r\n",
							"    if first_statement.startswith('CREATE VIEW'):\r\n",
							"        return True\r\n",
							"    return False\r\n",
							"\r\n",
							"def move_table_view(db_name, tbl_name, local_table_ddl):\r\n",
							"    ddl_statement = get_ddl_by_keyword_group(local_table_ddl)\r\n",
							"    if is_ddl_a_view(ddl_statement):\r\n",
							"        dst_local_ddl = os.path.join(metastore_view_dir, db_name, tbl_name)\r\n",
							"        os.rename(local_table_ddl, dst_local_ddl)\r\n",
							"        return True\r\n",
							"    return False\r\n",
							"\r\n",
							"def get_path_option_if_available(stmt):\r\n",
							"    # parse the OPTIONS keyword and pull out the `path` parameter if it exists\r\n",
							"    import re\r\n",
							"    params = re.search(r'\\((.*?)\\)', stmt).group(1) # \r\n",
							"    params_list = list(map(lambda p: p.lstrip().rstrip(), params.split(','))) # \r\n",
							"    for x in params_list:\r\n",
							"        if x.startswith('path'):\r\n",
							"            return f'OPTIONS ( {x} )' # \r\n",
							"    return ''    \r\n",
							"\r\n",
							"def is_table_location_defined(local_table_path): \r\n",
							"    \"\"\" check if LOCATION or OPTIONS(path ..) are defined for the table\r\n",
							"    \"\"\"\r\n",
							"    ddl_statement = get_ddl_by_keyword_group(local_table_path) \r\n",
							"    for keyword_param in ddl_statement: \r\n",
							"        if keyword_param.startswith('OPTIONS'): \r\n",
							"            options_param = get_path_option_if_available(keyword_param) \r\n",
							"            if options_param:\r\n",
							"                # if the return is not empty, the path option is provided which means its an external table\r\n",
							"                return True\r\n",
							"        elif keyword_param.startswith('LOCATION'):\r\n",
							"            # if LOCATION is defined, we know the external table location\r\n",
							"            return True\r\n",
							"    return False\r\n",
							"\r\n",
							"def update_table_ddl(local_table_path, db_path):\r\n",
							"    # check if the database location / path is the default DBFS path\r\n",
							"    table_name = os.path.basename(local_table_path) # local_table_path的basename就是table_name\r\n",
							"    is_db_default_path = db_path.startswith(default_database_location) #\r\n",
							"    if (not is_db_default_path) and (not is_table_location_defined(local_table_path)):\r\n",
							"        # the LOCATION attribute is not defined and the Database has a custom location defined\r\n",
							"        # therefore we need to add it to the DDL, e.g. dbfs:/db_path/table_name\r\n",
							"        table_path = os.path.join(db_path, table_name)\r\n",
							"        location_stmt = f\"\\nLOCATION '{table_path}'\"\r\n",
							"        with open(local_table_path, 'a') as fp:\r\n",
							"            fp.write(location_stmt)\r\n",
							"        return True\r\n",
							"    return False\r\n",
							"\r\n",
							"def is_delta_table(local_path):\r\n",
							"    with open(local_path, 'r') as fp:\r\n",
							"        for line in fp:\r\n",
							"            lower_line = line.lower()\r\n",
							"            if lower_line.startswith('using delta'):\r\n",
							"                return True\r\n",
							"    return False\r\n",
							"\r\n",
							"def get_local_tmp_ddl_if_applicable(current_local_ddl_path):\r\n",
							"    return current_local_ddl_path\r\n",
							"\r\n",
							"def apply_table_ddl(local_table_path, db_path):\r\n",
							"    \"\"\"\r\n",
							"    Run DDL command on destination workspace\r\n",
							"    :param local_table_path: local file path to the table DDL\r\n",
							"    :param db_path: database S3 / Blob Storage / ADLS path for the Database\r\n",
							"    \"\"\"\r\n",
							"    updated_table_status = update_table_ddl(local_table_path, db_path) # \r\n",
							"\r\n",
							"    # update local table ddl to a new temp file with OPTIONS and TBLPROPERTIES removed from the DDL for delta tables\r\n",
							"    if is_delta_table(local_table_path):\r\n",
							"        local_table_path = get_local_tmp_ddl_if_applicable(local_table_path)\r\n",
							"\r\n",
							"    with open(local_table_path, \"r\") as fp:\r\n",
							"        ddl_statement = fp.read()\r\n",
							"        #print(ddl_statement)\r\n",
							"        try:\r\n",
							"            ddl_results = spark.sql(ddl_statement)\r\n",
							"        except Exception as e:\r\n",
							"            if \"mstest\" in ddl_statement:\r\n",
							"                print(e)\r\n",
							"           \r\n",
							"def delete_dir_if_empty(local_dir):\r\n",
							"    \r\n",
							"    if os.path.exists(local_dir) and len(os.listdir(local_dir)) == 0:\r\n",
							"        os.rmdir(local_dir)           "
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"\r\n",
							"db_list = get_database_detail_dict()\r\n",
							"\r\n",
							"os.makedirs(metastore_view_dir, exist_ok=True)\r\n",
							"\r\n",
							"# recreate tables\r\n",
							"\r\n",
							"for db_name in db_list:\r\n",
							"    # create a dir to host the view ddl if we find them\r\n",
							"    os.makedirs(os.path.join(metastore_view_dir, db_name), exist_ok=True)\r\n",
							"    # get the local database path to list tables\r\n",
							"    local_db_path = os.path.join(metastore_dir, db_name)\r\n",
							"    # get a dict of the database attributes\r\n",
							"    database_attributes = all_db_details_json.get(db_name, '')\r\n",
							"    create_database_db(db_name, database_attributes)\r\n",
							"    db_path = database_attributes.get('Location')\r\n",
							"    tables = os.listdir(local_db_path)\r\n",
							"    print(tables)\r\n",
							"    for tbl_name in tables:\r\n",
							"        \r\n",
							"        local_table_ddl = os.path.join(local_db_path, tbl_name)\r\n",
							"        if tbl_name == \"mstest22\":\r\n",
							"            print(local_table_ddl)\r\n",
							"        if not move_table_view(db_name, tbl_name, local_table_ddl):\r\n",
							"            # we hit a table ddl here, so we apply the ddl\r\n",
							"            is_successful = apply_table_ddl(local_table_ddl, db_path)\r\n",
							"        else:\r\n",
							"            print(f'Moving view ddl to re-apply later: {db_name}.{tbl_name}')   \r\n",
							"    delete_dir_if_empty(os.path.join(metastore_view_dir, db_name))\r\n",
							"\r\n",
							""
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# recreate views\r\n",
							"views_db_list = os.listdir(metastore_view_dir)\r\n",
							"for db_name in views_db_list:\r\n",
							"    local_view_db_path = os.path.join(metastore_view_dir, db_name)\r\n",
							"    database_attributes = all_db_details_json.get(db_name, '')\r\n",
							"    db_path = database_attributes.get('Location')\r\n",
							"    if os.path.isdir(local_view_db_path):\r\n",
							"        views = os.listdir(local_view_db_path)\r\n",
							"        for view_name in views:\r\n",
							"            local_view_ddl = os.path.join(metastore_view_dir, db_name, '/', view_name)\r\n",
							"            is_successful = apply_table_ddl(local_view_ddl, db_path)\r\n",
							""
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/mstest')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "sparkpool2",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"runAsWorkspaceSystemIdentity": false,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "02fe24d5-09f4-4237-a209-fa2233f5184d"
					}
				},
				"metadata": {
					"saveOutput": true,
					"synapse_widget": {
						"version": "0.1",
						"state": {
							"1caae626-3e32-4f1f-896b-ffff3ab2da95": {
								"type": "Synapse.DataFrame",
								"sync_state": {
									"table": {
										"rows": [
											{
												"0": "Start Time",
												"1": "Duration",
												"2": "Run ID",
												"3": "Name",
												"4": "Source Type",
												"5": "Source Name",
												"6": "User",
												"7": "Status",
												"8": "max_feat",
												"9": "maxdepth",
												"10": "num_trees",
												"11": "mse"
											},
											{
												"0": "2022-08-25 17:15:45",
												"1": "3.9s",
												"2": "25a9b95a7a264417befc2d64aee182e2",
												"4": "NOTEBOOK",
												"5": "/Shared/202208/2208200030000022Venkata/LogRunsToExperiment",
												"6": "huangzhi@microsoft.com",
												"7": "FINISHED",
												"8": "3",
												"9": "6",
												"10": "100",
												"11": "3316.782078014208"
											},
											{
												"0": "2022-08-25 16:18:07",
												"1": "4.0s",
												"2": "0e19e2d23499442ebec2bc246d7ec368",
												"4": "NOTEBOOK",
												"5": "/Shared/202208/2208200030000022Venkata/LogRunsToExperiment",
												"6": "huangzhi@microsoft.com",
												"7": "FINISHED",
												"8": "3",
												"9": "6",
												"10": "100",
												"11": "3307.8569859555355"
											}
										],
										"schema": [
											{
												"key": "0",
												"name": "_c0",
												"type": "string"
											},
											{
												"key": "1",
												"name": "_c1",
												"type": "string"
											},
											{
												"key": "2",
												"name": "_c2",
												"type": "string"
											},
											{
												"key": "3",
												"name": "_c3",
												"type": "string"
											},
											{
												"key": "4",
												"name": "_c4",
												"type": "string"
											},
											{
												"key": "5",
												"name": "_c5",
												"type": "string"
											},
											{
												"key": "6",
												"name": "_c6",
												"type": "string"
											},
											{
												"key": "7",
												"name": "_c7",
												"type": "string"
											},
											{
												"key": "8",
												"name": "_c8",
												"type": "string"
											},
											{
												"key": "9",
												"name": "_c9",
												"type": "string"
											},
											{
												"key": "10",
												"name": "_c10",
												"type": "string"
											},
											{
												"key": "11",
												"name": "_c11",
												"type": "string"
											}
										],
										"truncated": false
									},
									"isSummary": false,
									"language": "scala"
								},
								"persist_state": {
									"view": {
										"type": "details",
										"chartOptions": {
											"chartType": "bar",
											"aggregationType": "count",
											"categoryFieldKeys": [
												"0"
											],
											"seriesFieldKeys": [
												"0"
											],
											"isStacked": false
										}
									}
								}
							}
						}
					},
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/76ebc6d1-182c-411a-9b4d-aed0242f8a82/resourceGroups/huangzhitest/providers/Microsoft.Synapse/workspaces/huangzhitestsynapse/bigDataPools/sparkpool2",
						"name": "sparkpool2",
						"type": "Spark",
						"endpoint": "https://huangzhitestsynapse.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/sparkpool2",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net",
							"authHeader": null
						},
						"sparkVersion": "3.2",
						"nodeCount": 3,
						"cores": 4,
						"memory": 28,
						"extraHeader": null
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"# spark.conf.set(\"spark.storage.synapse.huangzhistoragelogs.dfs.core.windows.net.linkedServiceName\", \"AzureDataLakeStorage1\")\r\n",
							"# spark.conf.set(\"fs.azure.account.auth.type.huangzhistoragelogs.dfs.core.windows.net\", \"SAS\")\r\n",
							"# spark.conf.set(\"fs.azure.sas.token.provider.type.huangzhistoragelogs.dfs.core.windows.net\", \"com.microsoft.azure.synapse.tokenlibrary.LinkedServiceBasedSASProvider\")\r\n",
							"\r\n",
							"\r\n",
							"# df = spark.read.csv(\"abfss://dataset@huangzhistoragelogs.dfs.core.windows.net/mstest\")\r\n",
							"\r\n",
							"# display(df)"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 46
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"\r\n",
							"# spark.conf.set(\"spark.storage.synapse.huangzhistoragelogs.dfs.core.windows.net.linkedServiceName\", \"AzureDataLakeStorage1\")\r\n",
							"# spark.conf.set(\"fs.azure.account.auth.type.huangzhistoragelogs.dfs.core.windows.net\", \"SAS\")\r\n",
							"# spark.conf.set(\"fs.azure.sas.token.provider.type.huangzhistoragelogs.dfs.core.windows.net\", \"com.microsoft.azure.synapse.tokenlibrary.LinkedServiceBasedSASProvider\")\r\n",
							"\r\n",
							"\r\n",
							"# df = spark.read.csv(\"abfss://dataset@huangzhistoragelogs.dfs.core.windows.net/mstest\")\r\n",
							"\r\n",
							"# display(df)\r\n",
							"\r\n",
							""
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 47
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"\r\n",
							"spark.conf.set(\"spark.storage.synapse.huangzhistoragelogs.dfs.core.windows.net.linkedServiceName\", \"AzureDataLakeStorage1\")\r\n",
							"spark.conf.set(\"fs.azure.account.oauth.provider.type.huangzhistoragelogs.dfs.core.windows.net\",\"com.microsoft.azure.synapse.tokenlibrary.LinkedServiceBasedTokenProvider\")\r\n",
							"\r\n",
							"df = spark.read.csv(\"abfss://dataset@huangzhistoragelogs.dfs.core.windows.net/mstest\")\r\n",
							"\r\n",
							"display(df)\r\n",
							"\r\n",
							""
						],
						"attachments": null,
						"outputs": [],
						"execution_count": 1
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							""
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/mstest2')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"nbformat": 4,
				"nbformat_minor": 2,
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"runAsWorkspaceSystemIdentity": false,
					"conf": {
						"spark.autotune.trackingId": "be7263d9-1f9a-43b6-822d-011b84e2dd97",
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.dynamicAllocation.minExecutors": "2"
					}
				},
				"metadata": {
					"enableDebugMode": false,
					"saveOutput": true,
					"synapse_widget": {
						"version": "0.1"
					},
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "python"
					},
					"language_info": {
						"name": "python"
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"print(hello world)"
						],
						"attachments": null,
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/sparkpool')]",
			"type": "Microsoft.Synapse/workspaces/bigDataPools",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"autoPause": {
					"enabled": true,
					"delayInMinutes": 15
				},
				"autoScale": {
					"enabled": false,
					"maxNodeCount": 0,
					"minNodeCount": 0
				},
				"nodeCount": 3,
				"nodeSize": "Medium",
				"nodeSizeFamily": "MemoryOptimized",
				"sparkVersion": "3.2",
				"isComputeIsolationEnabled": false,
				"sessionLevelPackagesEnabled": true,
				"annotations": []
			},
			"dependsOn": [],
			"location": "eastasia"
		},
		{
			"name": "[concat(parameters('workspaceName'), '/sparkpool2')]",
			"type": "Microsoft.Synapse/workspaces/bigDataPools",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"autoPause": {
					"enabled": true,
					"delayInMinutes": 15
				},
				"autoScale": {
					"enabled": false,
					"maxNodeCount": 0,
					"minNodeCount": 0
				},
				"nodeCount": 3,
				"nodeSize": "Small",
				"nodeSizeFamily": "MemoryOptimized",
				"sparkVersion": "3.2",
				"isComputeIsolationEnabled": false,
				"sessionLevelPackagesEnabled": true,
				"annotations": []
			},
			"dependsOn": [],
			"location": "eastasia"
		}
	]
}