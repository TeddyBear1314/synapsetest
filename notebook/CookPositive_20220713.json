{
	"name": "CookPositive_20220713",
	"properties": {
		"nbformat": 4,
		"nbformat_minor": 2,
		"sessionProperties": {
			"driverMemory": "28g",
			"driverCores": 4,
			"executorMemory": "28g",
			"executorCores": 4,
			"numExecutors": 2,
			"runAsWorkspaceSystemIdentity": false,
			"conf": {
				"spark.dynamicAllocation.enabled": "false",
				"spark.dynamicAllocation.minExecutors": "2",
				"spark.dynamicAllocation.maxExecutors": "2",
				"spark.autotune.trackingId": "32659328-7e12-4f97-9983-faddc64a17e7"
			}
		},
		"metadata": {
			"saveOutput": true,
			"synapse_widget": {
				"version": "0.1"
			},
			"enableDebugMode": false,
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "python"
			},
			"language_info": {
				"name": "python"
			},
			"sessionKeepAliveTimeout": 30
		},
		"cells": [
			{
				"cell_type": "code",
				"source": [
					"%%configure -f\r\n",
					"{\r\n",
					"    \"driverMemory\": \"368g\",\r\n",
					"    \"driverCores\": 64,\r\n",
					"    \"executorMemory\": \"368g\",\r\n",
					"    \"numExecutors\": 14,\r\n",
					"    \"conf\": {\r\n",
					"        \"spark.speculation\": \"true\",\r\n",
					"        \"spark.stop.improvement.enabled\": \"false\",\r\n",
					"        \"spark.network.timeout\": 10000000, \r\n",
					"        \"spark.executor.heartbeatInterval\": 10000000,\r\n",
					"        \"spark.dynamicAllocation.maxExecutors\" : \"14\",\r\n",
					"        \"spark.dynamicAllocation.enable\": \"true\",\r\n",
					"        \"spark.dynamicAllocation.minExecutors\": \"2\"        \r\n",
					"    }\r\n",
					"}"
				],
				"attachments": null,
				"execution_count": 1
			},
			{
				"cell_type": "code",
				"source": [
					"import numpy as np\r\n",
					"import pandas as pd\r\n",
					"import os\r\n",
					"\r\n",
					"import pyspark\r\n",
					"import pyspark.sql\r\n",
					"from pyspark.sql import DataFrame, SparkSession, functions, types, SQLContext\r\n",
					"from pyspark.sql.functions import col, date_add, expr, format_number, udf, when, hash, abs, pandas_udf, PandasUDFType\r\n",
					"from pyspark.sql.functions import when, expr\r\n",
					"from pyspark.sql.functions import * \r\n",
					"from pyspark.sql.functions import arrays_zip, col, explode, split\r\n",
					"from pyspark.sql import functions as F\r\n",
					"\r\n",
					"from pyspark.sql.types import * \r\n",
					"from pyspark.storagelevel import StorageLevel \r\n",
					"import scipy\r\n",
					"from sklearn import linear_model \r\n",
					"\r\n",
					"\r\n",
					"from datetime import datetime\r\n",
					"from datetime import timedelta\r\n",
					"\r\n",
					"from functools import reduce\r\n",
					"\r\n",
					"import datetime \r\n",
					"import dateutil\r\n",
					"\r\n",
					"from notebookutils import mssparkutils"
				],
				"attachments": null,
				"execution_count": 2
			},
			{
				"cell_type": "code",
				"source": [
					"#spark.conf.get(\"spark.speculation\")"
				],
				"attachments": null,
				"execution_count": 3
			},
			{
				"cell_type": "code",
				"source": [
					"%run \"/chenjzha/data_utils_v2\""
				],
				"attachments": null,
				"execution_count": 4
			},
			{
				"cell_type": "code",
				"metadata": {
					"tags": [
						"parameters"
					]
				},
				"source": [
					"timeBegin = ''\r\n",
					"timeEnd = ''\r\n",
					"maxLeadingHour = ''\r\n",
					"engineeringHourRange = ''"
				],
				"attachments": null,
				"execution_count": 5
			},
			{
				"cell_type": "code",
				"source": [
					"timeBegin = dateutil.parser.parse(timeBegin)\r\n",
					"timeEnd = dateutil.parser.parse(timeEnd)\r\n",
					"outputPathSuffix = f\"({timeBegin.year}_{timeBegin.month}_{timeBegin.day})({timeEnd.year}_{timeEnd.month}_{timeEnd.day})L{maxLeadingHour}h\""
				],
				"attachments": null,
				"execution_count": 7
			},
			{
				"cell_type": "code",
				"source": [
					"try:\r\n",
					"    mssparkutils.fs.mount(  \r\n",
					"        \"abfss://featuredata@onelinedfpadls2.dfs.core.windows.net\",  \r\n",
					"        \"/data\",\r\n",
					"        {\"linkedService\":\"onlinefeaturedata\"} \r\n",
					"    ) \r\n",
					"except:\r\n",
					"    print(\"already exist mount point\")\r\n",
					"    pass"
				],
				"attachments": null,
				"execution_count": 8
			},
			{
				"cell_type": "code",
				"source": [
					"KUSTO_CLUSTER = 'https://o365exoha.kusto.windows.net'\r\n",
					"KUSTO_DATABASE = 'O365ExoHAMon'\r\n",
					"KUSTO_DATABASE_TEST = 'O365ExoHAMonPPE'\r\n",
					"\r\n",
					"KUSTO_AAD_APP_ID = \"4cdae237-e303-4aa2-a1c4-ff067ca0b662\"\r\n",
					"KUSTO_AAD_APP_SECRET = mssparkutils.credentials.getSecretWithLS(linkedService=\"AzureKeyVault1\", secret=\"EXO-TORUS-APP-SECRET\")\r\n",
					"KUSTO_AAD_AUTHORITY_ID = \"cdc5aeea-15c5-4db6-b079-fcadd2505dc2\"\r\n",
					"\r\n",
					"\r\n",
					"kusto_loader = spark.read \\\r\n",
					"    .format(\"com.microsoft.kusto.spark.synapse.datasource\") \\\r\n",
					"    .option(\"kustoAadAppId\", KUSTO_AAD_APP_ID) \\\r\n",
					"    .option(\"kustoAadAppSecret\", KUSTO_AAD_APP_SECRET)  \\\r\n",
					"    .option(\"kustoAadAuthorityID\", KUSTO_AAD_AUTHORITY_ID) \\\r\n",
					"    .option(\"kustoCluster\", KUSTO_CLUSTER) \\\r\n",
					"    .option(\"kustoDatabase\", KUSTO_DATABASE)"
				],
				"attachments": null,
				"execution_count": 9
			},
			{
				"cell_type": "code",
				"source": [
					"sampleID = 2\r\n",
					""
				],
				"attachments": null,
				"execution_count": 10
			},
			{
				"cell_type": "code",
				"source": [
					"def getDiskFailureV4(startDate,endDate):\r\n",
					"    startDate = startDate.strftime(\"%Y-%m-%d\")\r\n",
					"    endDate = endDate.strftime(\"%Y-%m-%d\")\r\n",
					"    query = '''let R = RepairBoxDiskFailures_v4_basedV3\r\n",
					"| where CreatedTime < todatetime('endDate')\r\n",
					"| where CreatedTime >= todatetime('startDate')\r\n",
					"| where isnotempty(TicketId) and isnotempty(SerialNumber) and Data !contains \"Missing physicaldrive\"\r\n",
					"| project SerialNumber,MachineName,CreatedTime;\r\n",
					"R'''\r\n",
					"    query = query.replace('startDate',startDate)\r\n",
					"    query = query.replace('endDate',endDate)\r\n",
					"    kustoDF = kusto_loader.option(\"kustoQuery\", query).load()\r\n",
					"    kustoDF = kustoDF.withColumnRenamed('MachineName', 'env_cloud_roleInstance')\r\n",
					"    return kustoDF"
				],
				"attachments": null,
				"execution_count": 11
			},
			{
				"cell_type": "code",
				"source": [
					"def getDiskFailureV3orV2(startDate,endDate):\r\n",
					"    startDate = startDate.strftime(\"%Y-%m-%d\")\r\n",
					"    endDate = endDate.strftime(\"%Y-%m-%d\")\r\n",
					"    query = '''let R = RepairBoxDiskFailures_v3\r\n",
					"| where CreatedTime >= todatetime('startDate')\r\n",
					"| where CreatedTime < todatetime('endDate')\r\n",
					"| where isnotempty(SerialNumber) and DiskLocation contains 'HDD'\r\n",
					"| project env_cloud_roleInstance = MachineName, SerialNumber, Type, CreatedTime, DiskLocation\r\n",
					"| join kind=fullouter (\r\n",
					"    RepairBoxDiskFailures_v2\r\n",
					"    | where CreatedTime >= todatetime('startDate')\r\n",
					"    | where CreatedTime < todatetime('endDate')\r\n",
					"    | where isnotempty(SerialNumber) and LabelLocation contains 'HDD'\r\n",
					"    | project env_cloud_roleInstance, SerialNumber, Type, CreatedTime, DiskLocation = LabelLocation\r\n",
					"    )\r\n",
					"    on env_cloud_roleInstance, CreatedTime, DiskLocation and Type\r\n",
					"| project   env_cloud_roleInstance = iff(isnotempty(env_cloud_roleInstance), env_cloud_roleInstance, env_cloud_roleInstance1),\r\n",
					"            SerialNumber = iff(isnotempty(SerialNumber), SerialNumber, SerialNumber1),\r\n",
					"            CreatedTime = iff(isnotempty(CreatedTime), CreatedTime, CreatedTime1);\r\n",
					"R'''\r\n",
					"    query = query.replace('startDate',startDate)\r\n",
					"    query = query.replace('endDate',endDate)\r\n",
					"    kustoDF = kusto_loader.option(\"kustoQuery\", query).load()\r\n",
					"    return kustoDF"
				],
				"attachments": null,
				"execution_count": 12
			},
			{
				"cell_type": "code",
				"source": [
					"def getDiskFailureV5(startDate,endDate):\r\n",
					"    startDate = startDate.strftime(\"%Y-%m-%d\")\r\n",
					"    endDate = endDate.strftime(\"%Y-%m-%d\")\r\n",
					"    query = '''let R = RepairBoxDiskFailures_v5\r\n",
					"| where CreatedTime >= todatetime('startDate')\r\n",
					"| where CreatedTime < todatetime('endDate')\r\n",
					"| where isnotempty(SerialNumber) and DiskLocation contains 'HDD'\r\n",
					"| project env_cloud_roleInstance = MachineName, SerialNumber, CreatedTime;\r\n",
					"R'''\r\n",
					"    query = query.replace('startDate',startDate)\r\n",
					"    query = query.replace('endDate',endDate)\r\n",
					"    kustoDF = kusto_loader.option(\"kustoQuery\", query).load()\r\n",
					"    return kustoDF"
				],
				"attachments": null,
				"execution_count": 13
			},
			{
				"cell_type": "code",
				"source": [
					"def If_Blob_exist(hourly_time, path_prefix = \"\"):\r\n",
					"    path_suffix = f\"{path_prefix}/{hourly_time.year}/{hourly_time.month}/{hourly_time.day}/{hourly_time.hour}\"\r\n",
					"    # print(path_suffix)\r\n",
					"    jobId = mssparkutils.env.getJobId() \r\n",
					"    # print(jobId)\r\n",
					"    blob_exist =  os.path.exists(f\"/synfs/{jobId}/data/predictiondata/{path_suffix}\")\r\n",
					"    #print(blob_exist, f\"{jobId}/data/predictiondata/{path_suffix}\")\r\n",
					"    return blob_exist, f\"{jobId}/data/predictiondata/{path_suffix}\"\r\n",
					"\r\n",
					"def save_Blob(KustoDF, pathSuffix):\r\n",
					"\r\n",
					"    jobId = mssparkutils.env.getJobId() \r\n",
					"    # print(jobId)  \r\n",
					"    KustoDF.coalesce(1).write.mode(\"overwrite\").csv(f\"synfs:/{jobId}/data/test/Cooked/Positive/{pathSuffix}\",header=True)\r\n",
					"    "
				],
				"attachments": null,
				"execution_count": 14
			},
			{
				"cell_type": "code",
				"source": [
					"def extractRawData(diskFailures, pathPrefix):\r\n",
					"    dataBeginTime = timeBegin - datetime.timedelta(hours = maxLeadingHour + engineeringHourRange)\r\n",
					"    timeDiff = timeEnd - timeBegin\r\n",
					"    hourTotal = timeDiff.days * 24 + maxLeadingHour + engineeringHourRange\r\n",
					"    \r\n",
					"    pathList = []\r\n",
					"    for i in range(0, hourTotal):\r\n",
					"        currentTime = dataBeginTime + datetime.timedelta(hours=i)\r\n",
					"        #print(currentTime)\r\n",
					"        #pathSuffix = f\"{currentTime.year}/{currentTime.month}/{currentTime.day}/{currentTime.hour}\"\r\n",
					"        isExist, current_path = If_Blob_exist(currentTime, pathPrefix)\r\n",
					"        if(isExist):\r\n",
					"            pathList.append(current_path)\r\n",
					"        else:\r\n",
					"            print(f'{isExist} {current_path}')\r\n",
					"            #mssparkutils.notebook.exit(0)\r\n",
					"\r\n",
					"    filePaths = export_dirs_to_paths(pathList)\r\n",
					"\r\n",
					"    pySpark = SparkSession.builder.appName(\"ReadPySpark\").getOrCreate()   \r\n",
					"    rawData = pySpark.read.csv(path=filePaths, header=True, inferSchema=True)\r\n",
					"\r\n",
					"    rawData = rawData.withColumn(\"BinPreciseTimeStamp\", to_timestamp(col(\"BinPreciseTimeStamp\")))\r\n",
					"    rawData = rawData.withColumn('BinPreciseTimeStamp', date_trunc('hour', rawData.BinPreciseTimeStamp))\r\n",
					"\r\n",
					"    rawData = diskFailures.join(rawData, on = ['env_cloud_roleInstance', 'SerialNumber', 'BinPreciseTimeStamp'], how = 'inner')\r\n",
					"\r\n",
					"    #rawData = rawData.repartition('env_cloud_roleInstance', 'SerialNumber', 'FirstPreciseTimeStamp')\r\n",
					"\r\n",
					"    rawData = rawData.drop_duplicates().replace(float('nan'), None)\r\n",
					"    rawData = rawData.withColumn(\"SampleID\", lit(sampleID)).withColumn(\"Label\", lit(True))\r\n",
					"\r\n",
					"    return rawData"
				],
				"attachments": null,
				"execution_count": 15
			},
			{
				"cell_type": "code",
				"source": [
					"def resampleDiskFailure(diskFailures):\r\n",
					"    diskFailures = diskFailures.withColumn('CreatedTime', date_trunc('hour', diskFailures.CreatedTime))\r\n",
					"    diskFailures = diskFailures.withColumnRenamed('CreatedTime', 'FirstPreciseTimeStamp')\r\n",
					"    resampledDiskFailures = diskFailures.alias('resampledDiskFailures')\r\n",
					"\r\n",
					"    for leadingTime in range(1, maxLeadingHour):\r\n",
					"        diskFailures = diskFailures.withColumn('FirstPreciseTimeStamp', diskFailures.FirstPreciseTimeStamp - F.expr('INTERVAL 1 HOURS'))\r\n",
					"        resampledDiskFailures = resampledDiskFailures.unionByName(diskFailures, allowMissingColumns = False)\r\n",
					"\r\n",
					"    resampledDiskFailures = resampledDiskFailures.withColumn('BinPreciseTimeStamp', resampledDiskFailures.FirstPreciseTimeStamp)\r\n",
					"    diskFailures = resampledDiskFailures.alias('diskFailures')\r\n",
					"\r\n",
					"    for dataHour in range(1, engineeringHourRange):\r\n",
					"        diskFailures = diskFailures.withColumn('BinPreciseTimeStamp', diskFailures.BinPreciseTimeStamp - F.expr('INTERVAL 1 HOURS'))\r\n",
					"\r\n",
					"        resampledDiskFailures = resampledDiskFailures.unionByName(diskFailures, allowMissingColumns = False)\r\n",
					"    \r\n",
					"    # FirstPreciseTimeStamp is moved forward 2h in prediction, and moved back 2h before feature engineering.\r\n",
					"    resampledDiskFailures = resampledDiskFailures.withColumn('FirstPreciseTimeStamp', resampledDiskFailures.FirstPreciseTimeStamp + F.expr('INTERVAL 2 HOURS'))\r\n",
					"\r\n",
					"    resampledDiskFailures = resampledDiskFailures.repartition('env_cloud_roleInstance', 'SerialNumber', 'FirstPreciseTimeStamp')\r\n",
					"\r\n",
					"    return resampledDiskFailures"
				],
				"attachments": null,
				"execution_count": 16
			},
			{
				"cell_type": "code",
				"source": [
					"diskFailures = getDiskFailureV5(timeBegin, timeEnd).drop_duplicates()"
				],
				"attachments": null,
				"execution_count": 17
			},
			{
				"cell_type": "code",
				"source": [
					"resampledDiskFailures = resampleDiskFailure(diskFailures)"
				],
				"attachments": null,
				"execution_count": 18
			},
			{
				"cell_type": "code",
				"source": [
					"def getFeatureSMART():\r\n",
					"    rawSmart = extractRawData(resampledDiskFailures, 'DiskSmartAttributeHourly')\r\n",
					"\r\n",
					"    rawSmart = rawSmart.filter(rawSmart['AttributeName'].isNotNull())\r\n",
					"\r\n",
					"    # TODO: emergency fix 20211230 \r\n",
					"    rawSmart = rawSmart.withColumn('AttributeName', regexp_replace('AttributeName', 'Power-off_Retract_Count', 'Power-Off_Retract_Count'))\r\n",
					"\r\n",
					"    # may contain incorrect env_cloud_name == O365Passive, which will result in duplicated entries, one for each env_cloud_name, of the same disk in the engineered feature.\r\n",
					"    rawSmart = rawSmart.withColumn('env_cloud_name', lit(''))\r\n",
					"\r\n",
					"    #save_Blob(rawSmart, 'RawSMART')\r\n",
					"\r\n",
					"    featureSMART = get_common_features(None, rawSmart, smart_split_rows, SMART_Atts, ResultSchema)\r\n",
					"\r\n",
					"    return featureSMART\r\n",
					""
				],
				"attachments": null,
				"execution_count": 19
			},
			{
				"cell_type": "code",
				"source": [
					"featureSMART = getFeatureSMART()"
				],
				"attachments": null,
				"execution_count": 20
			},
			{
				"cell_type": "code",
				"source": [
					"#mssparkutils.notebook.exit(0)"
				],
				"attachments": null,
				"execution_count": 21
			},
			{
				"cell_type": "code",
				"source": [
					"def getFeaturePC():\r\n",
					"    rawPC = extractRawData(resampledDiskFailures, 'WindowsDiskPerfCounterHourly')\r\n",
					"    #save_Blob(rawPC, 'RawPC')\r\n",
					"    featurePC = get_perf_counter_features(None, rawPC, perf_counter_split_rows, PerfCounterFinalAttrNames, ResultSchema)\r\n",
					"    featurePC = featurePC.withColumnRenamed(\"env_cloud_name\", \"env_cloud_name_p\")\r\n",
					"    return featurePC"
				],
				"attachments": null,
				"execution_count": 22
			},
			{
				"cell_type": "code",
				"source": [
					"featurePC = getFeaturePC()"
				],
				"attachments": null,
				"execution_count": 23
			},
			{
				"cell_type": "code",
				"source": [
					"def getFeatureEvent():\r\n",
					"    rawEvent = extractRawData(resampledDiskFailures, 'DiskEventCounter')\r\n",
					"\r\n",
					"    # may contain incorrect env_cloud_name == O365Passive, which will result in duplicated entries, one for each env_cloud_name, of the same disk in the engineered feature.\r\n",
					"    rawEvent = rawEvent.withColumn('env_cloud_name', lit(''))\r\n",
					"\r\n",
					"    #save_Blob(rawEvent, 'RawEvent')\r\n",
					"\r\n",
					"    featureEvent = get_event_features(None, rawEvent)\r\n",
					"    featureEvent = featureEvent.withColumnRenamed(\"env_cloud_name\", \"env_cloud_name_e\")\r\n",
					"\r\n",
					"    return featureEvent"
				],
				"attachments": null,
				"execution_count": 24
			},
			{
				"cell_type": "code",
				"source": [
					"featureEvent = getFeatureEvent()"
				],
				"attachments": null,
				"execution_count": 25
			},
			{
				"cell_type": "code",
				"source": [
					"#mssparkutils.notebook.exit(0)"
				],
				"attachments": null,
				"execution_count": 26
			},
			{
				"cell_type": "code",
				"source": [
					"featureSmartPc = featureSMART.join(featurePC, on = ['env_cloud_roleInstance', 'SerialNumber', 'FirstPreciseTimeStamp', 'SampleID', 'Label'], how = 'full')\r\n",
					"featureSmartPcEvent = featureSmartPc.join(featureEvent, on = ['env_cloud_roleInstance', 'SerialNumber', 'FirstPreciseTimeStamp', 'SampleID', 'Label'], how = 'full')\r\n",
					"featureSmartPcEvent = featureSmartPcEvent.replace(float('nan'), None)"
				],
				"attachments": null,
				"execution_count": 27
			},
			{
				"cell_type": "code",
				"source": [
					"for column in selected_column_names_HDFV1:\r\n",
					"    if column not in featureSmartPcEvent.columns:\r\n",
					"        featureSmartPcEvent = featureSmartPcEvent.withColumn(column, lit(''))\r\n",
					"        print(f'add missing column {column}')\r\n",
					"featureSmartPcEvent = featureSmartPcEvent.select(selected_column_names_HDFV1)"
				],
				"attachments": null,
				"execution_count": 28
			},
			{
				"cell_type": "code",
				"source": [
					"#featureSmartPcEvent.coalesce(1).write.mode(\"overwrite\").format(\"com.databricks.spark.csv\").option(\"header\", \"true\").save(f\"dbfs:/mnt/featuredatablob/Dataset/20220520/{outputPathSuffix}\")\r\n",
					"\r\n",
					"#save_Blob(featureSmartPcEvent, outputPathSuffix)"
				],
				"attachments": null,
				"execution_count": 29
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"def getGen7():\r\n",
					"    query = '''let R = MachineInfo\r\n",
					"| where SKU contains 'Gen7'\r\n",
					"| distinct MachineName\r\n",
					"| project env_cloud_roleInstance = MachineName;\r\n",
					"R'''\r\n",
					"    kustoDF = kusto_loader.option(\"kustoQuery\", query).load()\r\n",
					"    return kustoDF\r\n",
					"\r\n",
					"\r\n",
					"def removeGen7(disks):\r\n",
					"    gen7 = getGen7()\r\n",
					"    disks = disks.join(gen7, on = ['env_cloud_roleInstance'], how = 'leftanti')\r\n",
					"    return disks"
				],
				"attachments": null,
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"featureSmartPcEvent = removeGen7(featureSmartPcEvent)\r\n",
					"featureSmartPcEvent = featureSmartPcEvent.select(selected_column_names_HDFV1)"
				],
				"attachments": null,
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"save_Blob(featureSmartPcEvent, outputPathSuffix)"
				],
				"attachments": null,
				"execution_count": null
			}
		]
	}
}